{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear Physics Approach Anomalous Phenomena in E<18 Ke V Hydrogen Ion Beam Implantation Experiments on Pd and Ti WANG Tieshan* PIAO Yubo** HAO Jifang* WANG Xuezhi** JIN Genming* and NIU Zhanqi** *Institute of Modern Physics, Chinese Academy of Sciences Lanzhou 730000, P.R. China **Department of Modern Physics of Lanzhou University Lanzhou 730000, P.R. China\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import openai\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"./data\"\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "title_abstract_list = []\n",
    "print(pdf_files)\n",
    "\n",
    "# testDir=\"./data/doc16.pdf\"\n",
    "\n",
    "# print(extract_title_abstract(testDir)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_abstract(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[0]\n",
    "\n",
    "        text = page.extract_text(x_tolerance=1, y_tolerance=5)\n",
    "\n",
    "        # print(page.chars)\n",
    "        table={}\n",
    "        for x,i in enumerate(page.chars):\n",
    "            # print(i[\"text\"])\n",
    "            # if i[\"text\"]==\" \" or i[\"text\"]==\"\\n\":\n",
    "            #     print(\"new line\")\n",
    "            if i[\"size\"] not in table:\n",
    "                table[i[\"size\"]]=\"\"\n",
    "            if x>0 and i[\"x0\"]-page.chars[x-1][\"x1\"]>=1:\n",
    "                table[i[\"size\"]]+=\" \"\n",
    "            if x>0 and page.chars[x-1][\"y1\"]-i[\"y1\"]>5:\n",
    "                table[i[\"size\"]]+=\" \"\n",
    "            table[i[\"size\"]]+=i[\"text\"]\n",
    "\n",
    "\n",
    "        # print(text,\"\\n\") \n",
    "        tmp=copy.deepcopy(table)\n",
    "\n",
    "        for i in tmp:\n",
    "            if len(tmp[i])<30:\n",
    "                del table[i]\n",
    "\n",
    "        title=table[max(table.keys())].strip().replace(\"  \",\" \")\n",
    "        abstract_match = re.search(r\"Abstract\\s+(.*?[.!?])\\s*\\n[c,Â©]\", text, re.DOTALL)\n",
    "        if not abstract_match:\n",
    "            abstract_match = re.search(r'(?i)abstract\\b\\.?:?]?\\s+((?:.|\\n)+?)(?=\\n\\d+\\.|Key|Introduction|[0,1]?\\Z)', text, re.DOTALL)\n",
    "\n",
    "        if abstract_match:\n",
    "            abstract = abstract_match.group(1).strip()\n",
    "\n",
    "    if not abstract_match:\n",
    "        abstract = \"No abstract found\"\n",
    "    # abstract = text.split('\\n')\n",
    "    # print(\"Title: \",title,\"Abstract: \",abstract)\n",
    "    # print(title, \"\\n\")\n",
    "\n",
    "    return title, abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_text_from_pdf(pdf_path):\n",
    "    allText=\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "             allText+=page.extract_text(x_tolerance=1, y_tolerance=5)\n",
    "    return allText\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words=word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered= [lemmatizer.lemmatize(w.lower()) for w in words if not w in stop_words and w.isalpha()]\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings={}\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   return openai.Embedding.create(input = text, model=model)['data'][0]['embedding']\n",
    "\n",
    "# embs=get_embedding(\"\\n\".join(filtered))\n",
    "\n",
    "for i in pdf_files:\n",
    "    text=get_text_from_pdf(\"./data/\"+i)\n",
    "    filtered=\" \".join(preprocess_text(text))\n",
    "    if len(filtered)>8192:\n",
    "        filtered=filtered[:8192]\n",
    "    embeddings[i]=get_embedding(filtered)\n",
    "\n",
    "# aaa=get_text_from_pdf(\"./data/doc26.pdf\")\n",
    "# aaa2=preprocess_text(aaa)\n",
    "# print(aaa2)\n",
    "# aaa3=get_embedding(aaa2)\n",
    "# print(aaa3)\n",
    "# # get the mean of aaa3\n",
    "# print(np.mean(aaa3,axis=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding=[np.mean(np.array(embeddings[i]), axis=0) for i in embeddings]\n",
    "avg_embedding=np.array(avg_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8469229574089849\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(embeddings[\"doc11.pdf\"],embeddings[\"doc16.pdf\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 11, 12, 18, 24]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_indices = random.sample(range(len(pdf_files)), 5)\n",
    "random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embeddings = np.array([embeddings[pdf_files[i]] for i in random_indices])\n",
    "random_documents = [pdf_files[i] for i in random_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents to Document doc1.pdf are:\n",
      "Document: doc8.pdf Similarity: 0.923830645284825\n",
      "Document: doc2.pdf Similarity: 0.917085091935906\n",
      "Document: doc7.pdf Similarity: 0.9124823522910969\n",
      "Most similar documents to Document doc2.pdf are:\n",
      "Document: doc1.pdf Similarity: 0.917085091935906\n",
      "Document: doc28.pdf Similarity: 0.9075754244280272\n",
      "Document: doc30.pdf Similarity: 0.8940143192454763\n",
      "Most similar documents to Document doc20.pdf are:\n",
      "Document: doc23.pdf Similarity: 0.9193531869487045\n",
      "Document: doc7.pdf Similarity: 0.918974585433316\n",
      "Document: doc19.pdf Similarity: 0.9169125824110804\n",
      "Most similar documents to Document doc26.pdf are:\n",
      "Document: doc28.pdf Similarity: 0.9361660868718512\n",
      "Document: doc27.pdf Similarity: 0.9075055382574022\n",
      "Document: doc29.pdf Similarity: 0.9065079389940269\n",
      "Most similar documents to Document doc31.pdf are:\n",
      "Document: doc29.pdf Similarity: 0.8995850871082629\n",
      "Document: doc27.pdf Similarity: 0.892087128155389\n",
      "Document: doc28.pdf Similarity: 0.8863542297222105\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(random_documents,random_embeddings):\n",
    "    # Exclude self-similarity by setting it to a negative value\n",
    "    print('Most similar documents to Document', i, 'are:')\n",
    "    table={}\n",
    "    for k,l in embeddings.items():\n",
    "        if k!=i:\n",
    "            table[k]=cosine_similarity(j,l)\n",
    "    table=sorted(table.items(), key=lambda x: x[1], reverse=True)\n",
    "    for k in table[:3]:\n",
    "        print(\"Document:\",k[0],\"Similarity:\",k[1])\n",
    "\n",
    "    # similarities = cosine_similarity(j.reshape(1,-1), avg_embedding)[0]\n",
    "    # most_similar_indices = np.argsort(similarities)[-3:]\n",
    "    # for j, idx in enumerate(reversed(most_similar_indices), 1):\n",
    "    #     print(f'{j}. Document {idx + 1} (Similarity: {similarities[idx]:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "w2VModel = Word2Vec(filtered, min_count = 1, vector_size = 100, window = 5, workers = 4)\n",
    "\n",
    "\n",
    "all_vectors = [w2V.wv[word] for word in model.wv.key_to_index]\n",
    "mean_vector = np.mean(all_vectors, axis=0)\n",
    "\n",
    "print(\"Mean vector:\", mean_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
