{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear Physics Approach Anomalous Phenomena in E<18 Ke V Hydrogen Ion Beam Implantation Experiments on Pd and Ti WANG Tieshan* PIAO Yubo** HAO Jifang* WANG Xuezhi** JIN Genming* and NIU Zhanqi** *Institute of Modern Physics, Chinese Academy of Sciences Lanzhou 730000, P.R. China **Department of Modern Physics of Lanzhou University Lanzhou 730000, P.R. China\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import openai\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"./data\"\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "title_abstract_list = []\n",
    "print(pdf_files)\n",
    "\n",
    "# testDir=\"./data/doc16.pdf\"\n",
    "\n",
    "# print(extract_title_abstract(testDir)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_abstract(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[0]\n",
    "\n",
    "        text = page.extract_text(x_tolerance=1, y_tolerance=5)\n",
    "\n",
    "        # print(page.chars)\n",
    "        table={}\n",
    "        for x,i in enumerate(page.chars):\n",
    "            # print(i[\"text\"])\n",
    "            # if i[\"text\"]==\" \" or i[\"text\"]==\"\\n\":\n",
    "            #     print(\"new line\")\n",
    "            if i[\"size\"] not in table:\n",
    "                table[i[\"size\"]]=\"\"\n",
    "            if x>0 and i[\"x0\"]-page.chars[x-1][\"x1\"]>=1:\n",
    "                table[i[\"size\"]]+=\" \"\n",
    "            if x>0 and page.chars[x-1][\"y1\"]-i[\"y1\"]>5:\n",
    "                table[i[\"size\"]]+=\" \"\n",
    "            table[i[\"size\"]]+=i[\"text\"]\n",
    "\n",
    "\n",
    "        # print(text,\"\\n\") \n",
    "        tmp=copy.deepcopy(table)\n",
    "\n",
    "        for i in tmp:\n",
    "            if len(tmp[i])<30:\n",
    "                del table[i]\n",
    "\n",
    "        title=table[max(table.keys())].strip().replace(\"  \",\" \")\n",
    "        abstract_match = re.search(r\"Abstract\\s+(.*?[.!?])\\s*\\n[c,Â©]\", text, re.DOTALL)\n",
    "        if not abstract_match:\n",
    "            abstract_match = re.search(r'(?i)abstract\\b\\.?:?]?\\s+((?:.|\\n)+?)(?=\\n\\d+\\.|Key|Introduction|[0,1]?\\Z)', text, re.DOTALL)\n",
    "\n",
    "        if abstract_match:\n",
    "            abstract = abstract_match.group(1).strip()\n",
    "\n",
    "    if not abstract_match:\n",
    "        abstract = \"No abstract found\"\n",
    "    # abstract = text.split('\\n')\n",
    "    # print(\"Title: \",title,\"Abstract: \",abstract)\n",
    "    # print(title, \"\\n\")\n",
    "\n",
    "    return title, abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_text_from_pdf(pdf_path):\n",
    "    allText=\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "             allText+=page.extract_text(x_tolerance=1, y_tolerance=5)\n",
    "    return allText\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words=word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered= [lemmatizer.lemmatize(w.lower()) for w in words if not w in stop_words and w.isalpha()]\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings={}\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "# embs=get_embedding(\"\\n\".join(filtered))\n",
    "\n",
    "for i in pdf_files:\n",
    "    text=get_text_from_pdf(\"./data/\"+i)\n",
    "    filtered=\"\\n\".join(preprocess_text(text))\n",
    "    if len(filtered)>8192:\n",
    "        filtered=filtered[:8192]\n",
    "    embeddings[i]=get_embedding(filtered)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[\"doc16.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding=[np.mean(np.array(embeddings[i]), axis=0) for i in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embedding=np.array(avg_embedding)\n",
    "avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/doc3.pdff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/nawaf/Desktop/a/Fall 23/pred/topic-modeling/model.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(allVecs, axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)  \u001b[39m# Represents all vectors' means for document\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m resultsW2V \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m text\u001b[39m=\u001b[39mget_text_from_pdf(\u001b[39m\"\u001b[39;49m\u001b[39m./data/doc3.pdff\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m filtered\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(preprocess_text(text))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m resultsW2V[\u001b[39m10\u001b[39m] \u001b[39m=\u001b[39m docVector(filtered)\n",
      "\u001b[1;32m/mnt/c/Users/nawaf/Desktop/a/Fall 23/pred/topic-modeling/model.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text_from_pdf\u001b[39m(pdf_path):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     allText\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mwith\u001b[39;00m pdfplumber\u001b[39m.\u001b[39;49mopen(pdf_path) \u001b[39mas\u001b[39;00m pdf:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m pdf\u001b[39m.\u001b[39mpages:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bkali-linux/mnt/c/Users/nawaf/Desktop/a/Fall%2023/pred/topic-modeling/model.ipynb#Y161sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m              allText\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mpage\u001b[39m.\u001b[39mextract_text(x_tolerance\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, y_tolerance\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pdfplumber/pdf.py:84\u001b[0m, in \u001b[0;36mPDF.open\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, repair)\u001b[0m\n\u001b[1;32m     82\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_fp, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPath)):\n\u001b[0;32m---> 84\u001b[0m     stream \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(path_or_fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m     stream_is_external \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(path_or_fp)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/doc3.pdff'"
     ]
    }
   ],
   "source": [
    "# Word2Vec \n",
    "\n",
    "def docVector(filteredSentence):\n",
    "    modelW2V = Word2Vec(sentences= filteredSentence, vector_size=len(filteredSentence), window=5, min_count=1, workers=4)\n",
    "    allVecs = [modelW2V.wv[word] for word in modelW2V.wv.key_to_index]\n",
    "    return np.mean(allVecs, axis = 0)  # Represents all vectors' means for document\n",
    "\n",
    "\n",
    "resultsW2V = {}\n",
    "\n",
    "text=get_text_from_pdf(\"./data/doc3.pdff\")\n",
    "filtered=\"\\n\".join(preprocess_text(text))\n",
    "resultsW2V[10] = docVector(filtered)\n",
    "\n",
    "resultsW2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Similarity Docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 20, 29, 23, 18]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_indices = random.sample(range(len(pdf_files)), 5)\n",
    "random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_embeddings = [avg_embedding[i] for i in random_indices]\n",
    "random_documents = [pdf_files[i] for i in random_indices]\n",
    "\n",
    "similarity=cosine_similarity(np.array(random_embeddings).reshape(-1,1), avg_embedding.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar documents to Document 1:\n",
      "1. Document 32 (Similarity: 1.0000)\n",
      "2. Document 31 (Similarity: 1.0000)\n",
      "3. Document 2 (Similarity: 1.0000)\n",
      "Most similar documents to Document 2:\n",
      "1. Document 32 (Similarity: 1.0000)\n",
      "2. Document 31 (Similarity: 1.0000)\n",
      "3. Document 2 (Similarity: 1.0000)\n",
      "Most similar documents to Document 3:\n",
      "1. Document 32 (Similarity: 1.0000)\n",
      "2. Document 31 (Similarity: 1.0000)\n",
      "3. Document 2 (Similarity: 1.0000)\n",
      "Most similar documents to Document 4:\n",
      "1. Document 32 (Similarity: 1.0000)\n",
      "2. Document 31 (Similarity: 1.0000)\n",
      "3. Document 2 (Similarity: 1.0000)\n",
      "Most similar documents to Document 5:\n",
      "1. Document 32 (Similarity: 1.0000)\n",
      "2. Document 31 (Similarity: 1.0000)\n",
      "3. Document 2 (Similarity: 1.0000)\n"
     ]
    }
   ],
   "source": [
    "for i, (document, similarities) in enumerate(zip(random_documents, similarity)):\n",
    "    # Exclude self-similarity by setting it to a negative value\n",
    "    similarities[random_indices[i]] = -1\n",
    "    most_similar_indices = np.argsort(similarities)[-3:]\n",
    "    print(f'Most similar documents to Document {i + 1}:')\n",
    "    for j, idx in enumerate(reversed(most_similar_indices), 1):\n",
    "        print(f'{j}. Document {idx + 1} (Similarity: {similarities[idx]:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "w2VModel = Word2Vec(filtered, min_count = 1, vector_size = 100, window = 5, workers = 4)\n",
    "\n",
    "\n",
    "all_vectors = [w2V.wv[word] for word in model.wv.key_to_index]\n",
    "mean_vector = np.mean(all_vectors, axis=0)\n",
    "\n",
    "print(\"Mean vector:\", mean_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
